---
title: "Adidas Quarterly Sales Forecasting"
subtitle: 'Schulich School of Business, Fall 2017'
author: "Kefei Wang"
date: "December 11, 2017"
output: html_document 
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Part A: Introduction

## Company Background

Adidas AG is a multinational corporation, founded and head-quartered in Herzogenaurach, Germany, that designs and manufactures shoes, clothing and accessories. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike. 
For this paper, I used the past quarterly sales data of ADIDAS to forecast its future sales from Q2 of 2017 to Q3 of 2018, applying Time Series Theories. 

## Data preparation

The data I used for forecasting is ADIDA's quarterly salses data from Q1, 2000 to Q1 2017. For mutivariable forecasting, I also included quearterly US GDP, EU GDP, China GDP, Producer Price Index: Fiber, Yarn, and Thread Mills: Carded Cotton Yarns and NIKE quarterly sales for the same time period.

### load packages
```{r message=FALSE, include=FALSE}
library(fpp)
library(knitr)
library(vars)
library(forecastHybrid)
library(rmarkdown)
```


### load data

```{r}
adidas_sales <- read.csv("C:/Schulich/econ6210/Project/adidas_revenue1.csv")
#adidas_sales <- read.csv("C:/Users/kofi2614/Desktop/adidas_revenue1.csv")


```

### rename data frame
```{r}
df=adidas_sales
```
 

### define data as time series
```{r}
df<-ts(df, start=c(2000,1), end=c(2017,1), frequency = 4)
```

### extract Adidas sales data
```{r}
y=df[,2]
```

#### set training data, test data
To evaluate the accuracy of forecasting methods, 20% of the data are held out as the test data set. 

```{r}
# set training data, test data
train<-window(y,start=c(2000,1), end=c(2013,4))

# test period
test<-window(y,start=2014)

# number of steps to forecast
h = length(test)

# out of sample forecast
h2 = 6
```


# Part B: Basic Data Exploration

### Observing Raw Dataset
```{r}
plot(y,main="Adidas Quarterly Sales", lwd=3)

###Centered moving average
#ma(y,order=9)
plot(y,main="Adidas Revenue", xlab="year", ylab="$millions", lwd=3)
lines(ma(y,9),col="orange",lwd=4)

#axis(1,at=seq(2000,2017,4))
```

Overall, the quarterly sales of ADIDAS have strong growing trends throughout the years except for a visible decrease in the year of 2004. There are also some fluctuations every year which indicates a possible seasonal pattern that increases in size as the level of the series increase.  
The centered moving average plot removes the influence of seasonality and makes the increasing trend more obviouse
These pattern indicate that a good forecast of this series would need to capture both the trend and seasonality.

### Seasonality visualization
```{r}
seasonplot(y,main="Seasonal plot: Adidas sales",
           year.labels = TRUE, year.labels.left = TRUE,
           col=1:20, pch=19,lwd=2)
monthplot(y, main="Seasonal plot: Adidas sales", 
          xlab="quarter", ylab = "$million")
```

Looking at the two plots above, ADIDAS performs differently in different seasons. It is clear that there is a large jump in sales in January each year. One possible explanation for this pattern is that, many popular sport leagues start new seasons in Q3 such as NFL and NBA. As a manufacture for sporting products, ADIDAS's sales have strong correlation with these games. The Seasonal graph also shows that there is a big gap between 2005 and 2006, meaning there was a huge increase between these two years. 

### Basic Statistics
```{r}
summary(y)
sd(y)
```

The basic statistics of the data shows that from 2000 to 2017, ADIDAS has average sales of 2,748 million US Dollars. The highest sales is $5,671 million which is generated in Q1 of 2017 and the lowest sales is $1,078 which is generated in Q4 of 2004. The standard deviation of the data is 1,112.64 which is relatively large, indicating a big variance. 

### Autocorrelation
```{r}
lag.plot(y,lags=9, do.lines = FALSE, lwd=4,main="Adidas Quarterly Sales")
```

The Scatter plots above shows that the data set have a strong linear relationship with its lags. 

```{r}
Acf(y, lwd=5, main="Adidas QUarterly Sales")
#Pacf(y, lwd=3)
```

The ACF graph indicates a strong trend component of the dataset. Although the tend pattern is getting weak as the number of lags increases, it is still significant when the lag is as large as 20 . Additionally, r4, r8, r12 and r16 is slightly higher than their neighbors. This is because the seasonal pattern of the data: the peaks tend to be four quarters apart (which is the Q3 of each year)

### Summary
Looking at the raw dataset, ADIDAS sales have a strong seasonal and increasing trend pattern. Out of four quarters, the third quarter generally has better performance. It also has a strong correlation with its lagged data. 

```{r}
y.diff1 = diff(train, differences = 1)
y.diff2 = diff(train, differences = 2)
plot(y.diff1)

summary(y)
sd(y)
plot(log(y),main="Transformed plot: Adidas sales" , lwd=5)
lambda<-BoxCox.lambda(y)
plot(BoxCox(y,lambda), main="Transformed plot: Adidas sales" , lwd=5)
Acf(log(y), main="transformed autocorrelation")


###Centered moving average
ma(y,order=9)
plot(y,main="Adidas Revenue", xlab="year", ylab="$millions", lwd=3)
lines(ma(y,9),col="orange",lwd=4)
### simple moving average
f9 <- rep(1/9, 9)
yma9 <- filter(y, f9, sides=1)
y.yma9 = cbind(y,yma9)
head(y.yma9)
tail(y.yma9)
plot(y, main="Magna quarterly sales and MA(9)",ylab="",xlab="",xaxt="n",yaxt="n", lwd=3)
lines(yma9,col="purple", lwd=3)
axis(1,at=c(seq(from=2000,to=2017,by=1)), las=2.,cex.axis = 1.1)
axis(2,cex.axis = 1.1)
legend("topleft",lty=1,col=c("black","purple"),
       legend=c("Actual","MA9"),bty="n",cex=1.1)


```
The row data set is obviously not stationary. Calculating difference using d = 1, the plot looks better but is still not stationary as p value of ADF test is still larger than 0.05. Try d = 2 and we finally got the station dataset. 

# Part C: Simple Forecasting Methods

### mean, naive, naive method
```{r}
fit.mean=meanf(train,h=h)
fit.naive=naive(train,h=h)
fit.snaive=snaive(train,h=h)

# plot with forecasts and actual values
plot(fit.mean, PI=FALSE,
     main="Forecasts for quarterly")
lines(fit.naive$mean,col=2)
lines(fit.snaive$mean,col=3)
lines(y)
legend("topleft",lty=1,col=c(4,2,3),
       legend=c("Mean method","Naive method","Seasonal naive method"),bty="n")


```

Comparing the forecasted data with test data, it is easy to conclude that neither Mean method nor naïve method can fit the test data well as they do not consider the effect of trend and seasonal patterns. Seasonal Naïve works well in the first two forecasting quarters as it follows the seasonal patter. However, as time series processes, the gap is getting bigger due to the increasing trend. 

### Linear Trend Method
```{r}
reg <- tslm(train ~ trend)
fit.tslm=forecast(reg, h=h,level=c(80,95))
summary(fit.tslm)
plot(fit.tslm, ylab="Adidas Quarterly Sales",
     xlab="t")
lines(fitted(reg),col="blue")
lines(y)
res.tslm <- ts(resid(fit.tslm))
acf(res.tslm)
###trend & seasonal
tps <- tslm(train ~ trend + season)
summary(tps)
fit.lmts = forecast(tps, h=h)
plot(fit.lmts)
lines(y)
res.lmts <- ts(resid(fit.lmts))
acf(res.lmts)
```

Based on Linear Trend Model, the forecasted quarterly sales = 1040.73 + 47.06t. It forms a straight line which perfectly catches the increasing trend pattern. 
By adding seasonality to the model, the forecasting looks even batter in the plot as it is almost overlapping with the test data. 

Looking at the ACF plots of both models, there are still some lags that are significant. It reveals that some autocorrelations are still not explained in the regression model and remain in the residuals, making the models have larger intervals.  

# Part D: Time Series Decomposition

In this Paper, I chose STL model to decompose the time series data.

### STL Decomposition
```{r}
y.stl <- stl(train, t.window=15, s.window="periodic", robust=TRUE)
summary(y.stl)
plot(y.stl)
```

Looking at the y.stl plot, the seasonal component and trend component are both pretty strong. Notice that seasonal component changes very slowly over time, meaning that throughout the years the seasonality have very similar patterns. The trend component indicates that the sales are increasing over time except for year 2008 where the plot is relatively flat. 

Looking at the gray bar on the right side of each panel, seasonal component shows the longest bar, indicating that seasonal component has smallest variance. And obviously trend component has largest variance. This finding can be very important for selecting the forecasting methods. Although the seasonal and trend components are both strong, as trend component has larger variance, the forecasting methods that consider trending pattern will generally work better than forecasting methods that consider seasonal pattern. We can verify this assumption by the following forecasting methods. 

### STL Forecasting
```{r}
fit.stl <- forecast(y.stl, method="rwdrift", h=h)
summary(fit.stl)
plot(fit.stl)
lines(y)
```

The model uses STL and random work drift to forecast. It follows the seasonal component pretty well but does not include trend component 

### Simple Exponential Smoothing
```{r}
fit.expo <- ses(train, h = h)
summary(fit.expo)
plot(fit.expo)
lines(y)
```

As we know the simple exponential smoothing has only one component: level. So from the forecaseting we can see a flat straight line.
This method is suitable for forecasting data with no trend or seasonal pattern. However, from our dataset the trend and seasonality are both very strong. So the forecasting plot looks as bad as Mean or Naïve method. 

###holt's linear trend method
```{r}
fit.hlinear <- holt(train, h=h)
summary(fit.hlinear)
plot(fit.hlinear, main = "Holt's Linear Trend")
lines(y)
```
alpha/beta


###holt's exponential trend method
```{r}
fit.het <- holt(train, exponential=TRUE, h=h)
summary(fit.het)
plot(fit.het)
lines(y)
```
###holt's damped trend method
```{r}
fit.hdt <- holt(train, damped=TRUE, h=h)
fit.hdtm <- holt(train, damped=TRUE, exponential = TRUE, h=h)
summary(fit.hdt)
summary(fit.hdtm)
plot(fit.hdt, col = 4)
lines(fit.hdtm$mean, col = 2)
lines(y)
legend("topleft",lty=1,col=c(4,2),
       legend=c("Addictive","Multiplicative"),bty="n")

```
The dataset shows a strong trend component which has relatively large variance, which means that forecasting method that include trending will have better accuracy. Although the seasonal component is also strong, the variance is much smaller than trend. 


### Holt Winter's  method
```{r}
fit.hwm <- hw(train, seasonal="multiplicative", h=h) 
fit.hwa <- hw(train, seasonal="additive", h=h) 
summary(fit.hwa)
summary(fit.hwm)
plot(fit.hwa, col = 4)
lines(fit.hwm$mean, col = 2)
lines(y)
legend("topleft",lty=1,col=c(4,2),
       legend=c("Addictive","Multiplicative"),bty="n")
#plot(fit.hwm)
#lines(y)
```
The dataset shows a strong trend component which has relatively large variance, which means that forecasting method that include trending will have better accuracy. Although the seasonal component is also strong, the variance is much smaller than trend. 
### ETS  method
```{r}
y.ets <- ets(train, model="ZZZ") 
summary(y.ets)
fit.ets <- forecast(y.ets, h=h)
summary(fit.ets)
plot(fit.ets)
lines(y)
```


###Arima method
```{r}
adf.test(train, alternative = "stationary")
kpss.test(train)
Acf(train)
Pacf(train)
y.arima <- auto.arima(train)
fit.arima <- forecast(y.arima, h=h)
summary(fit.arima)
plot(fit.arima)
lines(y)
```
The P value of ADF test is larger than 0.05 and of KPSS test is smaller than 0.05. Both of them indicates that the data is not stationary

Looking at the ACF plot, there is a decaying trend. 
There is a spike on lag 4 of PACF plot, and there is no spike thereafter. 
The two facts above indicates that there should be a ARIMA(p,d,0) model. The model generated automatically from auto.arima function complies with this conclusion. 

 ar1      ar2      ar3    drift
      0.9518  -0.0746  -0.2485  42.2554
s.e.  0.1348   0.1953   0.1367  15.2852
yt = 42.2554 + 0.9518*y't-1 - 0.0746*y't-2 - 0.2485y't-3



###Arima method for log(y)
```{r}
y.arima.lambda <- auto.arima(train, lambda=0)
fit.alog <- forecast(y.arima.lambda)
plot(fit.alog)
lines(y)
summary(fit.alog)
tsdiag(y.arima.lambda)
```

###nnetar
```{r}
net <- nnetar(train)
fit.ann <- forecast(net, h=h)
summary(fit.ann)
plot(fit.ann)
lines(y)
```

###bats
```{r}
tbats = tbats(train)
fit.bats <- forecast(tbats, h=h)
summary(fit.bats)
plot(fit.bats)
lines(y)

```

###hybrid
```{r}
hmod <- hybridModel(train, lambda = TRUE)
plot(hmod)
plot(hmod, type = "models")
fit.hmod <- forecast(hmod, h=h)
summary(fit.hmod)
plot(fit.hmod)
lines(y)

```


### forecast accuracy measures
```{r}
##########################################################
# accuracy measures
##########################################################

a.mean=accuracy(fit.mean, test)
a.naive=accuracy(fit.naive,test)
a.snaive=accuracy(fit.snaive,test)
a.expo=accuracy(fit.expo, test)
a.hlinear=accuracy(fit.hlinear, test)
a.het=accuracy(fit.het, test)
a.hdt=accuracy(fit.hdt, test)
a.hdtm=accuracy(fit.hdtm, test)
a.hwa=accuracy(fit.hwa, test)
a.hwm=accuracy(fit.hwm, test)
a.ets=accuracy(fit.ets, test)
a.stl=accuracy(fit.stl, test)
a.tslm=accuracy(fit.tslm, test)
a.lmts=accuracy(fit.lmts, test)
a.arima=accuracy(fit.arima, test)
a.alog=accuracy(fit.alog, test)
a.ann=accuracy(fit.ann, test)
a.bats=accuracy(fit.bats, test)
a.hmod=accuracy(fit.hmod, test)

```


### Combining forecast summary statistics into a table with row names
```{r}
a.table<-rbind(a.mean, a.naive, a.snaive, a.expo, a.hlinear, a.het, a.hdt, a.hdtm, a.hwa, a.hwm, a.ets, a.stl, a.tslm, a.lmts, a.arima, a.alog, a.ann,
               a.bats, a.hmod)

row.names(a.table)<-c('Mean training','Mean test', 'Naive training', 'Naive test', 'S. Naive training', 'S. Naive test' ,
                        'exponential smothing training', 'exponential smoothing test',"Holt's Linear trainning", "Holt's Linear test",  "Holt's exponential training", "Holt's exponential test", "Holt's Damped trainning", "Holt's Damped Test", "Holt's Damped exponential trainning", "Holt's Damped exponential Test",
                      'Holt-Winters addictive trainning', 'Holt-Winters addictive test','Holt-Winters multiplicative trainning', 'Holt-Winters multiplicative test', 'ETS training', 'ETS test', 'STL trainning', 'STL test', 'linear trend trainning', 'linear trend test', 'trend & seasonal trainning', 'trend & seasonal test', 
                      'ARIMA training', 'ARIMA test', 'Log Arima trainning', 'Log Arima test', 'NNETAR trainning', 'NNETAR test', 'BATS trainning', 'BATS test', 'Hybrid trainning', 'Hybrid test')
                     
# order the table according to MASE
a.table<-as.data.frame(a.table)
a.table<-a.table[order(a.table$MASE),]
a.table

```

###Summarize table
```{r}
kable(a.table, caption="Forecast accuracy")
```


###forecast using Arima model
```{r}

forc.all <- holt(y, exponential=TRUE, h=6)
plot(forc.all)
forc.all
```


### Vector autoregressions
```{r}
#train.var<-window(df,start=c(2000,1), end=c(2013,4))
#test.var<-window(df,start=2014)
vardata = log (df[,c(3,6,7,2)])
colnames(vardata) = c( "US_GDP", "Price_Index", "NIKE", "Adidas")
plot(vardata, main = "VAR data", xlab = "")
vs = VARselect(vardata, lag.max =9, season =4)
vs 
vs$selection[2]
var.1 = VAR(vardata, p=vs$selection[3], season =4)
roots(var.1)
serial.test(var.1, lags.pt = 16, type = "PT.adjusted")
acf(residuals(var.1), type="partial", lag.max=10)
Acf(residuals(var.1)[,1], main="ACF of US GDP equation residuals")
#Acf(residuals(var.1)[,2], main="ACF of Europe GDP equation residuals")
#Acf(residuals(var.1)[,3], main="ACF of China GDP Sales equation residuals")
Acf(residuals(var.1)[,2], main="ACF of Price Index equation residuals")
Acf(residuals(var.1)[,3], main="ACF of NIKE equation residuals")
Acf(residuals(var.1)[,4], main="ACF of Adidas equation residuals")

```

Notice that AIC, HQ, SC and FPE give the same results as 2, so we select 2 as the lag length.

Also the roots are all smaller than 1, and the residuals are randomly plotted in the ACF plots, so the model is good 

### granger causality
```{r}
causality(var.1, cause= c("US_GDP", "Price_Index", "NIKE" ))
```
According to the report, both P-values are larger than 0.05, so we fail to reject the null-hypothesis, which means that movements of GDP and Unemployment rate do not precede movements in HD sales. There is no statistical "causation" between GDP/unemployment rate and HD Sales 


### impulse responses
```{r}
var1a.irf <- irf(var.1,  n.ahead = 16, boot = TRUE,  runs=100, seed=99, cumulative=TRUE)#whole impact or one impact; recording 1:23

par(mfrow=c(2,2))
plot( irf(var.1, response = "Adidas", n.ahead = 24, boot = TRUE) , plot.type = "single")

par(mfrow=c(1,1))


```
Notice that 0 line is inside the 95% confidence interval lines all the time. It means that neither of GDP nor Unemployment rate has significant influence on change of HD sales with in all time lags. 

### fevd
```{r}
fevd(var.1, n.ahead = 16)

```

Based on the HD section, HD sales are mostly expained by itself.GDP and Unemployment rate do not have strong impact on HD Sales. For lag= 1, 94.83% of HD Sales is explained by itself, GDP and unemployment rate only explain 4.99% and 0.18%. For lag=24, the impact of GDP and unemployment rate increased to 15.49% and 10.30%. There is still 73.22% explained by itself.

### forecast
```{r}
var.fc = predict(var.1, n.ahead= 6)
plot(var.fc)

var.fc$fcst$Adidas
#one period forecast of Magna
exp (var.fc$fcst$Adidas)
#fit.var <- ts(df, start=c(2014,1), end=c(2017, 1), frequency = 4)
#a.var=accuracy(fit.var, test)
```
Forecast HD sales by 4 periods, HD sale from 2017:3 to 2018:2 are 24677.98, 22472.79, 24295.14, 28176.89. With in 95% confidence. 


